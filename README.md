# ANQA Platform

Monorepo for the ANQA ADHD screening platform with a Next.js frontend, FastAPI backend, and Traefik reverse proxy.

- Frontend: Next.js 15 + Tailwind CSS
- Backend: FastAPI (Python)
- Reverse proxy: Traefik v3 (HTTPS via Let’s Encrypt)
- Realtime media ingestion: Browser WebRTC → FastAPI (aiortc)
- Storage: Supabase Storage (encryption at rest) + Postgres metadata
- Packaging: Docker (multi-stage)
- Registry: GitHub Container Registry (GHCR)
- Host: Hetzner Cloud (Ubuntu 24.04), non-root `deploy` user

---




#### Launch local with git update!

docker compose -f infra/docker-compose.local.yml up --build




## HOW TO LAUNCH LATEST VERSION ON GIT!
Recommended: build from source on the server (ensures production `runner`)
```bash
ssh deploy@138.199.162.96
cd ~/anqa-platform/infra
bash ./deploy-latest.sh --update-repo
```





## HOW TO push .env to server via scp !!!
```bash
# From the repo root on your laptop:
scp infra/.env deploy@138.199.162.96:~/anqa-platform/infra/.env



# VIEW LOGS BACKEND
docker logs -f --tail=200 anqa-backend | cat
# or, via compose service name:
docker compose logs -f --tail=200 backend | cat


docker compose logs -f --tail=200 turn | cat

(base) user@local anqa-platform % curl -H "Authorization: Bearer e........" "https://anqa.cloud/api/webrtc/turn-credentials"
{"urls":["turn:turn.anqa.cloud:3478?transport=udp","turns:turn.anqa.cloud:5349?transport=tcp"],"username":"1.......","credential":"/1......=","ttl":36..}%                                                                         
(base) user@local anqa-platform % 

```




#### Launch local with git update!

```bash
## How to launch latest version Local!
docker compose -f infra/docker-compose.local.yml up --build

```




```bash
#Redeploy TURN and verify TLS:

docker compose logs -f --tail=200 turn | cat
Once 5349 is accepting TLS, retry a session with a fixed session_id and check:
"
Expect tmp_exists=true and tmp_size increasing while streaming; after stop, the close response should have non-null storage keys.
```







## What broke and why (Production 404 incident)

- Visiting `https://anqa.cloud/` returned HTTP 404, while `https://anqa.cloud/api/health` was OK.
- `anqa-frontend` kept restarting and wasn’t reachable by Traefik; Traefik logged the frontend service was ignored.
- `docker logs anqa-frontend` showed Next.js was starting with `next dev` then failing:
  “Couldn't find any pages or app directory.”

Root cause
- The `ghcr.io/anqa-digital-health/anqa-platform-frontend:main` image was built from the Dockerfile’s `dev` stage (runs `npm run dev`) instead of the production `runner` stage (runs `npm run start`).
- In production there is no source bind mount, so the `dev` stage crashes, the container restarts, and Traefik serves 404 because no healthy server exists for the frontend router.

Fix applied live
- Built a production frontend image on the server and recreated only the frontend service.
- Added a server-side compose override to force the `runner` stage and `npm run start`.

Prevention
- CI must build the frontend image from the final production stage (no `--target dev`).
- Sanity-check before rollout:
  ```bash
  docker image inspect ghcr.io/anqa-digital-health/anqa-platform-frontend:main \
    --format '{{json .Config.Cmd}}' | cat
  # Expect: ["npm","run","start"] (not dev)
  ```
- When building on the server, ensure compose uses the `runner` stage or deploy a known-good GHCR tag that you’ve verified has `start`.

---

## Environments

- Local development: `infra/docker-compose.local.yml` (no Traefik). Frontend on 3000, backend on 8000.
- Production: `infra/docker-compose.yml` + `infra/.env` (Traefik on 80/443, HTTPS, domains).

New backend setup (Supabase + app/core + secure WebRTC ingest)
- `apps/backend/app/core/config.py`: loads envs from repo root `.env` and also tries `apps/backend/supabase/.env` (generated by Supabase CLI). In development, if `SUPABASE_PROJECT_URL` is not set, it falls back to `http://host.docker.internal:54321`. For keys, it uses `SUPABASE_API_KEY` or falls back to `SUPABASE_SERVICE_ROLE_KEY`/`SUPABASE_ANON_KEY`. For JWT, it prefers `SUPABASE_JWT_SECRET` and falls back to `JWT_SECRET`.
- `apps/backend/app/core/supabase_client.py`: initializes a Supabase Python client at import time using values from `Settings()`.
- `apps/backend/app/main.py`: FastAPI app with CORS configured from `ALLOWED_ORIGINS`. `/health` returns `{"supabase": true}` when the client has a non-empty REST URL (it does not ping the DB). Includes router for `/webrtc/*` endpoints.
  - `apps/backend/app/endpoints/webrtc.py`: WebRTC ingestion over HTTPS signaling
    - `POST /webrtc/offer` (Content-Type: `application/sdp` or JSON) — accepts SDP offers from the browser, answers, forks encrypted media (DTLS-SRTP) into:
      - Server-side recorder (WebM VP8/Opus) via aiortc `MediaRecorder`
      - Python analysis relay workers (minimal stubs included)
    - `POST /webrtc/close?session_id=…` — finalizes recorder, extracts audio (`.wav`) via ffmpeg, uploads files to Supabase Storage.
  - Files are uploaded to bucket `recordings` by default (configurable via `SUPABASE_RECORDINGS_BUCKET`).
- `infra/docker-compose.local.yml`: sets `ENVIRONMENT=development`, bind-mounts the backend and frontend, and maps `host.docker.internal` so the backend can talk to a locally running Supabase CLI on the host (`54321`).

Developer flow (local):
1. Start Supabase locally with the CLI from `apps/backend/supabase` (see section below).
2. Run `docker compose -f infra/docker-compose.local.yml up --build`.
3. Visit `http://localhost:8000/health` and `http://localhost:3000`.



`infra/.env` (example)
```ini
ACME_EMAIL=you@your-domain.tld
DOMAIN_FRONTEND=anqa.cloud
DOMAIN_BACKEND=api.anqa.cloud
SUPABASE_PROJECT_URL=https://<YOUR_PROJECT_REF>.supabase.co
SUPABASE_API_KEY=<SERVICE_ROLE_KEY>
SUPABASE_PROJECT_ID=<YOUR_PROJECT_REF>
SUPABASE_JWT_SECRET=<JWT_SECRET>
# TURN (coturn) configuration
TURN_REALM=anqa.cloud
# Public IP of the host where TURN runs (same server as Traefik/backends)
TURN_EXTERNAL_IP=138.199.162.96
# Strong random secret (32+ chars) used to mint ephemeral creds (HMAC-SHA1)
TURN_STATIC_AUTH_SECRET=<RANDOM_HEX_OR_BASE64_SECRET>
# Optional explicit hostname override; by default we derive turn.${DOMAIN_FRONTEND}
# TURN_HOST=turn.anqa.cloud
# Timezone (optional)
TZ=UTC
```

DNS records (example)
```
anqa.cloud        A   138.199.162.96
api.anqa.cloud    A   138.199.162.96
# TURN subdomain for UDP/TCP exposure (not proxied by CDN)
turn.anqa.cloud   A   138.199.162.96
# Optional IPv6 (AAAA): 2a01:4f8:c013:495c::1
```

---

## Standard operations (Production)

Recommended: build from source on the server (ensures production `runner`)
```bash
ssh deploy@138.199.162.96
cd ~/anqa-platform/infra
bash ./deploy-latest.sh --update-repo
```
Notes:
- This script builds services from the compose definition. If a service lacks a `build:` section, it won’t be rebuilt.
- To guarantee the frontend uses the `runner` stage when building from source on the server, keep this override file on the server at `~/anqa-platform/infra/docker-compose.override.yml`:
```yaml
services:
  frontend:
    build:
      context: ../apps/frontend
      dockerfile: Dockerfile
      target: runner
    command: ["npm","run","start"]
```
- If you run compose manually, include the override: `docker compose -f docker-compose.yml -f docker-compose.override.yml ...`

Alternative: pull prebuilt images from GHCR
```bash
cd /home/deploy/anqa-platform
npm run prod:pull && npm run prod:up
```
Only do this if CI publishes the frontend image built from the production stage (verify `npm run start`).

Verification checklist
```bash
docker ps --format 'table {{.Names}}\t{{.Image}}\t{{.Status}}' | cat
curl -I https://anqa.cloud | cat
curl -I https://anqa.cloud/surveys | cat
curl -I https://anqa.cloud/surveys/clinician | cat
curl -k https://anqa.cloud/api/health | cat
curl -s -o /dev/null -w '%{http_code}\n' https://anqa.cloud/api/webrtc/offer | cat   # expect 405
curl -s -o /dev/null -w '%{http_code}\n' -X POST https://anqa.cloud/api/webrtc/close | cat  # expect 401
```
Expect HTTP/2 200 responses for pages and a 200 from `/api/health`.

Convenience alias (server)
```bash
echo 'alias anqa-deploy="bash ~/anqa-platform/infra/deploy-latest.sh --update-repo"' >> ~/.bashrc && source ~/.bashrc
```

---

## Runbook: Homepage 404

1) Inspect state
```bash
docker ps | cat
docker logs --tail 200 anqa-frontend | cat
docker logs --tail 200 traefik | cat
```

2) If frontend logs show Next.js “pages/app directory” error
- The container is likely running `npm run dev`.

Quick hotfix
```bash
ssh deploy@138.199.162.96 <<'EOF'
set -e
cd ~/anqa-platform/apps/frontend
docker build -t ghcr.io/anqa-digital-health/anqa-platform-frontend:main .
cd ~/anqa-platform/infra
docker compose up -d --force-recreate --no-deps frontend
EOF
```

3) Confirm container command and networking
```bash
docker inspect anqa-frontend --format '{{json .Config.Cmd}}' | cat   # expect ["npm","run","start"]
docker network inspect web | cat                                     # frontend, backend, traefik present
```

4) Verify
```bash
curl -I https://anqa.cloud | cat
```

Hardening recommendations
- Remove duplicate frontend router rules in `infra/docker-compose.yml`; keep only one rule like: `Host(${DOMAIN_FRONTEND}) && !PathPrefix(/api)`.
- Add a frontend healthcheck to fail fast if the process exits:
```yaml
services:
  frontend:
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000/ >/dev/null || exit 1"]
      interval: 5min+
      timeout: 10s
      retries: 3
```

---

## Local development

From the repo root
```bash
npm run dev          # build & run (frontend:3000, backend:8000)
npm run dev:detached
npm run logs
npm run stop
```

Local media streaming (MVP)
1) Start local stack:
```bash
docker compose -f infra/docker-compose.local.yml up --build
```
2) Open the prototype page at `http://localhost:3000/screening/prototype`.
3) Click “Start secure stream” and allow camera + microphone.
4) Click “Stop & save” to finalize and upload to Supabase Storage.

Notes
- The browser media is encrypted in transit (DTLS-SRTP). Signaling runs over HTTPS.
- Files are saved as `sessions/<session_id>/recording.webm` and `sessions/<session_id>/audio.wav` in the `recordings` bucket.
- Ensure your local `.env` has Supabase variables (see Supabase setup below). The backend uses the service key to upload.

Local API rewrites for the frontend
```ini
# apps/frontend/.env.local
SET_API_TO_LOCAL=1
API_LOCAL_TARGET=http://localhost:8000
```

---

## Repository structure

```
.
├── apps
│   ├── backend
│   │   ├── app/main.py
│   │   ├── app/endpoints/webrtc.py      # WebRTC ingest + recording + upload
│   │   ├── Dockerfile
│   │   └── requirements.txt
│   └── frontend
│       ├── src/app/{layout.tsx,page.tsx,globals.css}
│       ├── Dockerfile
│       ├── next.config.ts
│       ├── postcss.config.js
│       ├── tailwind.config.ts
│       ├── package.json
│       └── (optional) .env.local
├── infra
│   ├── docker-compose.yml          # production (Traefik, HTTPS, domains)
│   ├── docker-compose.local.yml    # local dev (no Traefik)
│   ├── traefik/traefik.yml
│   └── .env                        # production env (created by you)
└── package.json                    # root scripts (compose helpers)
```

---

## Security and operations notes

- SSH: keys-only; `deploy` user; disable root login.
- HTTPS: Let’s Encrypt HTTP-01. Ensure ports 80/443 are open.
- GHCR: either make images public or `docker login ghcr.io` with a PAT (`read:packages`).
- Rollback: redeploy a previous tag (e.g., a commit SHA) for fast recovery.

---

## Useful production commands

```bash
# Logs
docker compose -f ~/anqa-platform/infra/docker-compose.yml logs -f --tail=100 | cat
#/Users/juliuskleinle/Desktop/Anqa_Platform/anqa-platform/infra/docker-compose.yml

# Individual service logs
docker logs -f traefik | cat
docker logs -f anqa-frontend | cat
docker logs -f anqa-backend | cat

# Rebuild just one service from source
cd ~/anqa-platform/infra
docker compose build frontend && docker compose up -d --no-deps frontend | cat
docker compose build backend && docker compose up -d --no-deps backend | cat


#USE THIS ONE HERE FOR LOCAL DEV!!!
docker compose -f infra/docker-compose.local.yml up --build
```

Tips
- If the browser caches an older page, hard refresh or open `https://anqa.cloud/?v=$(date +%s)` once.
- CI should build production images (final stage). Avoid `--target dev` for prod tags.



















---

## Supabase setup (Anqa-db)

Local development
```bash
# Install CLI (macOS)
brew install supabase/tap/supabase
cd apps/backend/supabase
supabase login
supabase link --project-ref <YOUR_PROJECT_REF>
# Start local stack and reset DB from declarative schemas
supabase start
supabase db reset
```

Environment variables
```ini
# .env at repo root (copy from .env.example)
ENVIRONMENT=development
# From inside containers we reach the host via host.docker.internal
SUPABASE_PROJECT_URL=http://host.docker.internal:54321
SUPABASE_API_KEY=<service_or_anon_key_from_supabase_status>
SUPABASE_PROJECT_ID=<YOUR_PROJECT_REF>
SUPABASE_JWT_SECRET=<JWT_SECRET>
SUPABASE_RECORDINGS_BUCKET=recordings
```

Production (on server)
```ini
# infra/.env
ACME_EMAIL=you@your-domain.tld
DOMAIN_FRONTEND=anqa.cloud
DOMAIN_BACKEND=api.anqa.cloud
SUPABASE_PROJECT_URL=https://<YOUR_PROJECT_REF>.supabase.co
SUPABASE_API_KEY=<SERVICE_ROLE_KEY>
SUPABASE_PROJECT_ID=<YOUR_PROJECT_REF>
SUPABASE_JWT_SECRET=<JWT_SECRET>
```

Authentication (FastAPI + Supabase JWT)
- What was added
  - A lightweight middleware that decodes Supabase JWTs from `Authorization: Bearer <token>` and attaches a minimal `User` to `request.state.user`.
  - A protected `/me` endpoint demonstrating dependency-based auth.
  - An optional helper to use the user JWT with PostgREST so Supabase RLS applies automatically.

- Files
  - `apps/backend/app/core/auth_middleware.py`: parses/validates token via `decode_supabase_jwt` and sets `request.state.user` and `request.state.jwt`.
  - `apps/backend/app/main.py`: wires the middleware and exposes `/me` (protected).
  - `apps/backend/app/core/db.py`: `postgrest_for_request(request)` returns a PostgREST client authorized with the user’s JWT when present.

- Behavior
  - If a valid token is provided, `request.state.user` is set to `User(id, email)`, and `request.state.jwt` holds the raw token.
  - If no/invalid token, both remain `None`. Endpoints are public unless explicitly protected.
  - Token verification uses HS256 with the Supabase JWT secret and audience `authenticated`.

- Protecting endpoints
  - Use the existing dependency from `apps/backend/app/core/auth.py`:
    - `get_current_user(request)` raises 401 if no authenticated user is attached by the middleware.
  - Example usage in a route:
    ```python
    from fastapi import Depends
    from app.core.auth import get_current_user, User

    @app.get("/me")
    def me(user: User = Depends(get_current_user)) -> User:
        return user
    ```
  - You can also add the dependency at the router level to protect a group of routes.

- RLS-aware database access (optional)
  - Use the helper to automatically apply Supabase Row-Level Security when a user is authenticated:
    ```python
    from fastapi import Depends
    from starlette.requests import Request
    from app.core.db import postgrest_for_request
    from app.core.auth import get_current_user, User

    @app.get("/todos")
    def list_todos(request: Request, user: User = Depends(get_current_user)):
        pg = postgrest_for_request(request)
        return pg.from_("todos").select("*").execute().data
    ```
  - If no token is present, the helper returns the service-role client (RLS bypassed). Combine with the dependency above to ensure the route is protected.

- Configuration
  - `SUPABASE_JWT_SECRET` must match your Supabase project JWT secret. In local dev, `JWT_SECRET` from the Supabase CLI environment is also recognized by `Settings`.
  - CORS allowed origins are controlled by `ALLOWED_ORIGINS` (comma-separated).
  - Existing `DISABLE_AUTH=true` (if used with `check_request_user_id`) bypasses that specific check; do not use in production.

- Quick test
  ```bash
  # Public health (no token required)
  curl -s https://anqa.cloud/api/health | jq

  # Protected endpoint: supply a valid Supabase user JWT
  export JWT="<paste a valid Supabase access token>"
  curl -s -H "Authorization: Bearer $JWT" https://anqa.cloud/api/me | jq
  # Expect: { "id": "<uuid>", "email": "user@example.com" }
  ```

Health endpoint
- `/api/health` returns a JSON payload with `status`, `timestamp`, and `supabase`.
- `supabase: true` means the client was configured (URL present). It does not perform a database query. If you want a deeper readiness probe, add a fast Supabase call with a short timeout.

---























## Secure media streaming overview

Why WebRTC
- Sub-second latency; robust NAT traversal; automatic encryption (DTLS-SRTP).

Flow
1) User logs in via Supabase (JWT).
2) Frontend captures camera + mic, creates an SDP offer, posts it to `/api/webrtc/offer` with the JWT.
3) FastAPI answers and starts recording (`.webm`) while forking media to analysis workers.
4) On `/api/webrtc/close`, recording is finalized, `audio.wav` is extracted via ffmpeg, and both files are uploaded to Supabase Storage (encrypted at rest).

Security
- Signaling over HTTPS via Traefik + Let’s Encrypt.
- Media encrypted in transit (DTLS-SRTP); no application-layer E2EE to allow server-side analysis.
- Access to stored artifacts governed by Supabase RLS and signed URLs.

TURN/STUN (production hardening)
- Add coturn on TCP/443 (TURN-TLS) for restrictive networks and configure `iceServers` in the frontend.

---

## TURN server (coturn)

Purpose
- TURN relays media when peers cannot connect directly due to NAT/firewalls. We run coturn in its own container, outside Traefik, because WebRTC requires UDP and dynamic port ranges that L7 proxies can’t manage.

Architecture
- Service: `turn` (Docker, image: `instrumentisto/coturn`), exposed directly on host ports.
- Ports:
  - UDP 3478 (TURN over UDP)
  - TCP 3478 (TURN over TCP)
  - TCP 5349 (TURN over TLS — optional; enable after certs)
  - UDP 49160–49200 (relay range)
- DNS: `turn.${DOMAIN_FRONTEND}` (example: `turn.anqa.cloud`) → A record to the server public IP.
- Isolation: Not attached to the `web` network; not handled by Traefik.

Compose definitions
- `infra/docker-compose.yml` includes the `turn` service with the required port mappings and HMAC auth flags:
  - `--use-auth-secret` + `--static-auth-secret=${TURN_STATIC_AUTH_SECRET}`
  - `--external-ip=${TURN_EXTERNAL_IP}` (ensures public IP is advertised in candidates)
  - `--min-port`/`--max-port` for relay range
  - Commented cert mounts and flags to enable TLS later
- Backend service receives `DOMAIN_FRONTEND`, `TURN_STATIC_AUTH_SECRET`, (optional) `TURN_HOST` to mint credentials.

Environment variables
- `TURN_REALM`: Authentication realm (use your apex domain, e.g., `anqa.cloud`).
- `TURN_EXTERNAL_IP`: Public IPv4 of the server hosting TURN.
- `TURN_STATIC_AUTH_SECRET`: Strong random secret used to mint time-limited credentials (HMAC-SHA1).
- `TURN_HOST` (optional): Explicit hostname for the TURN server; by default, backend derives `turn.${DOMAIN_FRONTEND}`.

Backend endpoint (ephemeral credentials)
- `GET /api/webrtc/turn-credentials` (protected: requires authenticated user)
- Returns:
  ```json
  { "urls": ["turn:turn.anqa.cloud:3478?transport=udp"], "username": "<epoch_expiry>", "credential": "<hmac_base64>", "ttl": 3600 }
  ```
- The username is an expiry epoch; credential is HMAC-SHA1(secret, username), base64-encoded. Compatible with coturn’s REST auth.

Frontend wiring
- The prototype page fetches credentials at session start and sets `RTCPeerConnection({ iceServers })` accordingly. It falls back to Google STUN if fetching fails.

TLS for `turns:` (optional but recommended)
1) Obtain certs for `turn.${DOMAIN_FRONTEND}` (use DNS-01 to avoid port conflicts):
   ```bash
   sudo apt-get update && sudo apt-get install -y certbot python3-certbot-dns-hetzner
   sudo bash -c 'cat > /root/.secrets/hetzner.ini <<EOF\ndns_hetzner_api_token=YOUR_HETZNER_DNS_TOKEN\nEOF'
   sudo chmod 600 /root/.secrets/hetzner.ini
   sudo certbot certonly --dns-hetzner --dns-hetzner-credentials /root/.secrets/hetzner.ini \
     -d turn.${DOMAIN_FRONTEND} --agree-tos -m you@${DOMAIN_FRONTEND} --non-interactive
   ```
2) In `infra/docker-compose.yml`:
   - Uncomment the cert mount volumes under the `turn` service.
   - Uncomment the `--cert` and `--pkey` flags in the `turn` command.
3) Add `"turns:turn.${DOMAIN_FRONTEND}:5349"` to the backend’s returned URLs and redeploy.

Firewall and provider notes
- If you enable a cloud firewall (e.g., Hetzner), allow inbound:
  - UDP 3478
  - TCP 3478
  - TCP 5349
  - UDP 49160–49200
  The host currently has no OS firewall; Docker publishes required ports directly.

Deployment
```bash
ssh deploy@<server>
cd ~/anqa-platform/infra
# Ensure infra/.env contains TURN_* vars (see above)
./deploy-latest.sh turn backend
```

Verification
1) Trickle ICE test page: add `turn:turn.${DOMAIN_FRONTEND}:3478?transport=udp` plus username/password from `/api/webrtc/turn-credentials`; verify `typ relay` candidates.
2) Application: start a session on the prototype page and inspect `chrome://webrtc-internals` → selected candidate includes `relay`.
3) Server logs: `docker logs -f anqa-turn` should show ALLOCATE/REFRESH lines.

Troubleshooting
- No relay candidates: confirm `TURN_EXTERNAL_IP` is set and DNS resolves to the public IP; check UDP 3478 and relay range are reachable.
- 401 on `/turn-credentials`: ensure you’re authenticated (Supabase JWT) and the backend sees `TURN_STATIC_AUTH_SECRET`.
- Corporate networks: enable TLS and add `turns:...:5349`; some networks block UDP.






---

## Postmortem and Runbook: WebRTC recording not saved to Supabase (TURN/TLS)

### What happened
- Users could start a prototype session and obtain TURN credentials, but recordings never appeared in Supabase. The backend response from `/api/webrtc/close` had `storage_recording_key: null` and `storage_audio_key: null`.
- The server-side debug at `/api/webrtc/debug?session_id=...` (with Authorization header) showed `active: false` or `tmp_exists: false`, `tmp_size: 0`, meaning no media file was written by the recorder.
- Frontend logs showed `iceconnectionstate: checking → disconnected` and `connectionstate: failed`, indicating the relay path was not stable.

### Root causes (multiple)
- TURN over TLS (TURNS) was not actually available on port 5349. Only UDP/TCP on 3478 was exposed, which is often blocked on restrictive networks; connections died quickly, so no frames were recorded.
- Docker Compose misconfiguration for the `turn` service:
  - The `volumes:` section was accidentally commented out while the two PEM paths were left as list entries, so certs were not mounted. This caused coturn startup failures and later a restart loop.
  - An unsupported flag `--no-ipv6` was added; the image rejected it and printed usage before exiting.
  - `TURN_EXTERNAL_IP` was not present in `infra/.env` initially, so the container did not have a valid external/public IP to advertise.
  - Certbot issued certs at `live/turn.anqa.cloud-0001`, but the compose volumes pointed at `live/turn.anqa.cloud`. The latter had become empty directories earlier, breaking the mounts. Coturn then could not load certs.
- Backend returned only `turn:...3478?transport=udp` at first; TURNS URL was missing until env wiring was added, making many clients rely on UDP-only relay.
- Operator tried to call `/api/webrtc/debug` directly in the address bar (no Authorization header) and received 401; the endpoint is protected and requires a Bearer token.

### Fixes applied
- Backend improvements:
  - Added optional TURNS in `apps/backend/app/endpoints/webrtc.py` controlled by env flags `TURN_ENABLE_TURNS`/`TURN_ENABLE_TLS`. When set, the backend includes `turns:turn.<domain>:5349?transport=tcp` in the issued ICE servers.
  - Added logs for `on_track` and `connectionstatechange`, and slightly increased the recorder start wait before finalize to reduce race conditions.
- Compose and TURN service corrections (`infra/docker-compose.yml`):
  - Passed `TURN_ENABLE_TURNS` and `TURN_ENABLE_TLS` env vars into the backend service `environment:` so the API can emit TURNS URLs.
  - Fixed the `turn` service `volumes:` block and mounted the certs properly:
    ```yaml
    volumes:
      - /etc/letsencrypt/live/turn.${DOMAIN_FRONTEND}/fullchain.pem:/certs/fullchain.pem:ro
      - /etc/letsencrypt/live/turn.${DOMAIN_FRONTEND}/privkey.pem:/certs/privkey.pem:ro
    ```
  - Removed the unsupported `--no-ipv6` flag.
  - Ensured TLS flags are active:
    ```yaml
    - --cert=/certs/fullchain.pem
    - --pkey=/certs/privkey.pem
    ```
  - Ensured `TURN_EXTERNAL_IP` is set in `infra/.env` (public IPv4), and exposed ports are mapped: `3478/udp`, `3478/tcp`, `5349/tcp`, and `49160-49200/udp`.
  - For compatibility, used the more widely-documented short flag for external IP and explicit listeners when needed:
    ```yaml
    - -X ${TURN_EXTERNAL_IP}
    - --listening-port=3478
    - --tls-listening-port=5349
    ```
  - If certbot issued `turn.anqa.cloud-0001`, either point compose volumes to `...-0001/...` or create a symlink:
    ```bash
    sudo rm -rf /etc/letsencrypt/live/turn.anqa.cloud
    sudo ln -s /etc/letsencrypt/live/turn.anqa.cloud-0001 /etc/letsencrypt/live/turn.anqa.cloud
    ```
- Certificates:
  - Reissued a valid cert: `sudo certbot certonly --standalone -d turn.anqa.cloud --non-interactive --agree-tos -m <email>` while Traefik was stopped to free port 80, then verified non-empty PEM files.
  - Opened firewall for `5349/tcp`, `3478/udp`, `3478/tcp`, and relay range `49160-49200/udp`.
- Verification:
  - Confirmed TLS handshake to TURN: `openssl s_client -connect turn.anqa.cloud:5349 -servername turn.anqa.cloud -brief` → `CONNECTION ESTABLISHED`.
  - Confirmed API emits TURNS: `/api/webrtc/turn-credentials` includes `turns:turn.anqa.cloud:5349?transport=tcp`.
  - After re-test, server logs showed `on_track` and `recorder started`, and `/api/webrtc/debug` reported `tmp_exists: true` with growing `tmp_size` during the session. On stop, `/webrtc/close` returned non-null storage keys and files appeared in Supabase Storage under `recordings/sessions/<session_id>/`.

### How to debug quickly next time (checklist)
1) TURN/TLS health
   - Handshake: `openssl s_client -connect turn.<domain>:5349 -servername turn.<domain> -brief` → must connect.
   - Container ports: `docker compose logs turn` (look for certs found and listeners), `ss -lntup | grep 3478\|5349` inside the container.
   - Compose resolves correctly: `docker compose config | sed -n '/turn:/,/^[^ ]/p'` (inspect command, env, volumes).
   - DNS: A record for `turn.<domain>` points at the public IPv4.
2) Backend behavior
   - `GET /api/webrtc/turn-credentials` (with Authorization) contains `turns:...:5349?transport=tcp` when TURNS is enabled.
   - During a run, `docker compose logs backend | grep \[webrtc\]` shows `on_track` and `recorder started`.
   - While streaming, `GET /api/webrtc/debug?session_id=...` (with Authorization) shows `tmp_exists: true` and `tmp_size > 0`.
3) Finalization and upload
   - On stop, `/webrtc/close` should return non-null `storage_recording_key`/`storage_audio_key`.
   - If upload errors occur, test a simple upload from inside the backend container using the Supabase service role key.
4) Common pitfalls
   - Missing/incorrect cert mounts or using the wrong `live/` path (e.g., `...-0001`).
   - `TURN_EXTERNAL_IP` not set in `infra/.env`.
   - Missing `TURN_ENABLE_TURNS`/`TURN_ENABLE_TLS` in backend env so the API never emits `turns:`.
   - Fetching debug in the address bar (401). Always include `Authorization: Bearer <token>`.
   - Corporate networks blocking UDP: ensure TURNS is working; optionally add a client fallback to retry with `iceTransportPolicy: 'all'` after ~10s if the initial relay-only attempt does not connect.

### Command snippets (copy-paste)
- Verify TLS:
```bash
openssl s_client -connect turn.anqa.cloud:5349 -servername turn.anqa.cloud -brief </dev/null
```
- Ensure backend emits TURNS:
```bash
curl -H "Authorization: Bearer <TOKEN>" https://anqa.cloud/api/webrtc/turn-credentials | jq
```
- Debug a live session (replace session id and token):
```bash
curl -H "Authorization: Bearer <TOKEN>" \
  "https://anqa.cloud/api/webrtc/debug?session_id=<SESSION_ID>"
```

### Outcome
- TURN over TLS on 5349 is now up and reachable; the backend issues TURNS URLs; sessions remain connected and the aiortc recorder writes files. Finalization uploads to Supabase succeed, and keys are stored in the `screenings` table.















---



## Beginner-friendly explainer: certificates and TURN over TLS (TURNS)

If you’re new to servers and certificates, this section explains the why and how in simple terms.

### What is a certificate (TLS/SSL cert) and why do we need it?
- A TLS/SSL certificate is like a digital ID card for a website or service. It proves “I am turn.anqa.cloud” and sets up an encrypted connection.
- For WebRTC on restrictive networks, we relay media through a TURN server using TLS (encrypted) on port 5349. Browsers and operating systems will only trust this secure connection if the TURN server presents a valid certificate for the TURN hostname (here: `turn.anqa.cloud`).
- Without a valid certificate:
  - The encrypted connection (TURNS) cannot be established reliably.
  - Clients (browsers) may fail to connect or drop the connection quickly.

### Which certificate files do we need?
- We need two files on the server for `turn.anqa.cloud`:
  - `fullchain.pem`: The public certificate plus intermediate chain (what you show to clients).
  - `privkey.pem`: The private key (keep this secret).
- These come from a Certificate Authority (CA). We use Let’s Encrypt via `certbot`, which issues trusted certs for free.

### How do we get the certificate (step-by-step)?
1) Make sure DNS points `turn.anqa.cloud` to your server’s public IPv4.
2) Temporarily free port 80 so Let’s Encrypt can verify you control the domain:
   ```bash
   cd ~/anqa-platform/infra
   docker compose stop traefik
   ```
3) Ask Let’s Encrypt to issue the cert with certbot:
   ```bash
   sudo apt-get update && sudo apt-get install -y certbot
   sudo certbot certonly --standalone -d turn.anqa.cloud --non-interactive --agree-tos -m you@anqa.cloud
   ```
   - This creates files in `/etc/letsencrypt/live/turn.anqa.cloud/` (or sometimes `turn.anqa.cloud-0001/` if you’ve issued before).
4) Check the files exist and are not empty:
   ```bash
   sudo ls -lh /etc/letsencrypt/live/turn.anqa.cloud/fullchain.pem \
               /etc/letsencrypt/live/turn.anqa.cloud/privkey.pem
   ```
   - If certbot created a `turn.anqa.cloud-0001` folder, either update the compose volumes to point there, or create a symlink:
     ```bash
     sudo rm -rf /etc/letsencrypt/live/turn.anqa.cloud
     sudo ln -s /etc/letsencrypt/live/turn.anqa.cloud-0001 /etc/letsencrypt/live/turn.anqa.cloud
     ```
5) Mount these files into the TURN container (coturn). In `infra/docker-compose.yml` under the `turn` service, you should have:
   ```yaml
   volumes:
     - /etc/letsencrypt/live/turn.${DOMAIN_FRONTEND}/fullchain.pem:/certs/fullchain.pem:ro
     - /etc/letsencrypt/live/turn.${DOMAIN_FRONTEND}/privkey.pem:/certs/privkey.pem:ro
   ```
   And in the `command:` list (already set in the repo):
   ```yaml
   - --cert=/certs/fullchain.pem
   - --pkey=/certs/privkey.pem
   ```
6) Start Traefik again and restart the TURN service:
   ```bash
   docker compose up -d traefik turn
   docker compose logs --tail=100 turn | cat
   ```
7) Verify TLS on port 5349 is working:
   ```bash
   openssl s_client -connect turn.anqa.cloud:5349 -servername turn.anqa.cloud -brief </dev/null
   ```
   - You should see `CONNECTION ESTABLISHED` and the certificate CN/SAN for `turn.anqa.cloud`.

### Why does the certificate need to match the TURN hostname?
- The TURN URL we give the browser looks like `turns:turn.anqa.cloud:5349?transport=tcp`.
- The browser expects the server at that address to present a certificate for exactly that hostname. If it doesn’t match, the connection is considered untrusted and will be blocked.

### Common pitfalls (and fixes)
- “Connection refused” on 5349
  - TURN container not running, or certs not mounted, or firewall closed. Fix by mounting certs, opening `5349/tcp`, and restarting the `turn` service.
- Certificate files exist but empty
  - The `live/turn.anqa.cloud` folder is an empty stub. Use the `...-0001` folder from certbot, or create a symlink as shown above.
- TURN container keeps restarting with usage text
  - A flag is unsupported or missing required env (e.g., `TURN_EXTERNAL_IP`). Check `docker compose config` for the final command and env, remove unknown flags, and set `TURN_EXTERNAL_IP` in `infra/.env`.
- Debug endpoint returns 401
  - You must include your Supabase access token in the `Authorization: Bearer <token>` header when calling `/api/webrtc/debug`.

### Very short: what to remember as a beginner
- TURN over TLS (TURNS) needs a real certificate for `turn.anqa.cloud` to keep connections stable on locked-down networks.
- Use `certbot` to get `fullchain.pem` and `privkey.pem`, mount them into the TURN container, and confirm port 5349 accepts TLS.
- If a session still fails, check `/api/webrtc/debug` while streaming to see if the server is writing a temporary file.















## On testing the TURN Server: setup


I'll quickly verify the TLS certificate handshake on port 5349 and check if UDP 3478 responds from your machine. Then I’ll explain what the coturn logs indicate and how to properly test TURN (not via a browser URL).
```bash
openssl s_client -connect turn.anqa.cloud:5349 -servername turn.anqa.cloud </dev/null | head -n 40
```

I validated the TLS handshake on 5349—it presents a valid Let's Encrypt certificate for turn.anqa.cloud. Now I'll quickly check TCP 5349 and UDP 3478 reachability from your machine.
```bash
nc -vz turn.anqa.cloud 5349
```
```bash
nc -uvz turn.anqa.cloud 3478
```

I just verified your TURN endpoints from your Mac: TLS on 5349 presents a valid Let’s Encrypt cert for turn.anqa.cloud, and both 5349/TCP and 3478/UDP are reachable.

- Is the cert wrong? No. The server presents a valid cert for turn.anqa.cloud on 5349.
- Should you browse to turn.anqa.cloud in Chrome? No. TURN/TURNS aren’t HTTP; visiting the hostname in a browser is not a valid test and will look “broken.”
- Is the server working now? Yes. Port checks pass, and your API returns fresh TURN credentials. The coturn logs show normal scanner noise (connection resets) but also confirm TLS and cert files are loaded.

How to test correctly
- Use a WebRTC ICE test, e.g. Google’s Trickle ICE page, and paste:
  - URLs: turn:turn.anqa.cloud:3478?transport=udp, turns:turn.anqa.cloud:5349?transport=tcp
  - Username and password from your curl response
- Or use a TURN client (e.g., turnutils_uclient) to perform an allocation with the same creds.

If it “worked yesterday” but a client now fails
- TURN creds are ephemeral (TTL 3600). Fetch fresh credentials before each session.
- Ensure system clocks are in sync (HMAC auth is time-sensitive); clock skew causes 401/auth failures.
- If failures are network-specific, rely on 5349/TCP (TURNS) for restrictive networks; you already expose it and it’s reachable.

Optional hardening
- Consider configuring coturn with `external-ip=<public_ip>` if it’s behind NAT/Docker, though your current setup is functioning.

Summary
- Certs are correct and ports are open; the server is up.
- Don’t test via browser URL; test with WebRTC/ICE using the provided username/password.
- Most likely issues are expired creds or clock skew, not certificates.


